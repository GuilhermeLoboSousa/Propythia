{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ProPythia__\n",
    "\n",
    "This file is a simulation fro the antioxidant dataset;\n",
    "\n",
    "I intend to run propythia here so that in the future I can make comparisons with the results obtained with omnia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install propythia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i will do this simulation with the antioxidant data, where the data in unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from propythia.protein.descriptors import ProteinDescritors\n",
    "from propythia.protein.encoding import Encoding\n",
    "from propythia.ml.shallow_ml import ShallowML\n",
    "from propythia.protein.sequence import ReadSequence\n",
    "\n",
    "from propythia.ml.deep_ml import DeepML\n",
    "\n",
    "from propythia.feature_selection import FeatureSelection\n",
    "\n",
    "from propythia.preprocess import Preprocess\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antioxidant_data=pd.read_csv('data_antioxidant_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antioxidant_data[\"classe\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum and minimum length of strings in the specified column\n",
    "max_length = antioxidant_data[\"Seqs\"].str.len().max()\n",
    "min_length = antioxidant_data[\"Seqs\"].str.len().min()\n",
    "mean_seqs= antioxidant_data[\"Seqs\"].str.len().mean()\n",
    "num_sequences_over_600 = (antioxidant_data[\"Seqs\"].str.len() > 600).sum()\n",
    "\n",
    "print(f\"Number of sequences over 600 amino acids: {num_sequences_over_600}\")\n",
    "\n",
    "num_sequences_under_100 = (antioxidant_data[\"Seqs\"].str.len() < 100).sum()\n",
    "\n",
    "print(f\"Number of sequences under 100 amino acids: {num_sequences_under_100}\")\n",
    "\n",
    "print(f\"Maximum length: {max_length}\")\n",
    "print(f\"Minimum length: {min_length}\")\n",
    "print(f\"mean: {mean_seqs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = antioxidant_data[\"Seqs\"].str.len() > 350\n",
    "\n",
    "counts = antioxidant_data[mask].groupby(\"classe\").size()\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i think that is more complete, maybe need to change omnia or adapt omnia\n",
    "def pad_and_truncate_sequences(df, seq_col, max_length, padding_value='X'):\n",
    "    \"\"\"\n",
    "    Pad and truncate the protein sequences in a DataFrame to a specific length.\n",
    "\n",
    "    :param df: DataFrame containing the protein sequences.\n",
    "    :param seq_col: Name of the column in df that contains the protein sequences.\n",
    "    :param max_length: The maximum length for all sequences.\n",
    "    :param padding_value: The value to use for padding the sequences.\n",
    "    :return: DataFrame with the padded and truncated sequences.\n",
    "    \"\"\"\n",
    "    def pad_and_truncate(seq):\n",
    "        # Truncate the sequence if it's too long\n",
    "        if len(seq) > max_length:\n",
    "            seq = seq[:max_length]\n",
    "        # Pad the sequence if it's not long enough\n",
    "        elif len(seq) < max_length:\n",
    "            seq += padding_value * (max_length - len(seq))\n",
    "        return seq\n",
    "\n",
    "    df['padded_and_truncated_sequence'] = df[seq_col].apply(pad_and_truncate)\n",
    "    return df\n",
    "\n",
    "# Use the function\n",
    "antioxidant_data = pad_and_truncate_sequences(antioxidant_data, 'Seqs', 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enconde_df = Encoding(dataset= antioxidant_data ,  col= 'padded_and_truncated_sequence')\n",
    "nlf=enconde_df.get_nlf()\n",
    "print(np.array(nlf['nlf'][0]).shape)\n",
    "print(nlf['nlf'])\n",
    "\n",
    "expanded_arrays =  nlf['nlf'].apply(lambda x: np.array(x))\n",
    "X_nlf = np.array(expanded_arrays.tolist())\n",
    "\n",
    "y_nlf = nlf['classe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__esm__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enconde_df = Encoding(dataset= antioxidant_data ,  col= 'padded_and_truncated_sequence')\n",
    "\n",
    "esm = enconde_df.get_esm(v_esm='esm2_150')\n",
    "\n",
    "X_esm=esm ['esm']\n",
    "y_esm = esm['classe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__protbert__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enconde_df = Encoding(dataset= antioxidant_data ,  col= 'padded_and_truncated_sequence')\n",
    "protbert = enconde_df.get_protbert()\n",
    "\n",
    "X_protbert = protbert['protbert']\n",
    "y_protbert = protbert['classe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain all the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate features\n",
    "def calculate_feature(data):\n",
    "    list_feature = []\n",
    "    for seq in data['Seqs']:\n",
    "        res = {'sequence': seq}\n",
    "        sequence = ReadSequence()  # creating sequence object\n",
    "        ps = sequence.read_protein_sequence(seq)\n",
    "        protein = ProteinDescritors(ps)  # creating object to calculate descriptors\n",
    "        feature = protein.get_adaptable([44])\n",
    "        #feature_dict = feature.iloc[0].to_dict()\n",
    "        res.update(feature)\n",
    "        # feature = protein.get_all(lamda_paac=5, lamda_apaac=5) #minimal seq len = 5\n",
    "        # lambda should not be larger than len(sequence)\n",
    "        list_feature.append(res)\n",
    "    print('saving features')\n",
    "    antioxidant= pd.DataFrame(list_feature)\n",
    "    return antioxidant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antioxidant_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antioxidant_dataset=calculate_feature(antioxidant_data)\n",
    "antioxidant_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_descriptors=antioxidant_data[\"classe\"]\n",
    "X_descriptors=antioxidant_dataset.loc[:,antioxidant_dataset.columns != \"sequence\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split antioxidant_dataset X and Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_descriptors, X_test_descriptors, y_train_descriptors, y_test_descriptors = train_test_split(X_descriptors,y_descriptors, test_size=0.2,stratify=y_descriptors, random_state=42)\n",
    "X_train_nlf, X_test_nlf, y_train_nlf, y_test_nlf = train_test_split(X_nlf,y_nlf, test_size=0.2,stratify=y_nlf, random_state=42)\n",
    "#X_train_esm, X_test_esm, y_train_esm, y_test_esm = train_test_split(X_esm,y_esm, test_size=0.2,stratify=y_esm, random_state=42)\n",
    "#X_train_protbert, X_test_protbert, y_train_protbert, y_test_protbert = train_test_split(X_protbert,y_protbert, test_size=0.2,stratify=y_protbert, random_state=42)\n",
    "\n",
    "# standard scaler article does not refer scaling and do not validate in x_test, however, we do it anyway\n",
    "#scaler = StandardScaler().fit(X_train_descriptors)\n",
    "#X_train_descriptors = scaler.transform(X_train_descriptors)\n",
    "#X_test_descriptors = scaler.transform(X_test_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_x', X_train_nlf.shape)\n",
    "print('test_x', X_test_nlf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShallowML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiple_models(X_train, X_test, y_train, y_test,X,model_names=['svm','linear_svm','rf','gboosting','knn','sgd','lr','nn']):\n",
    "    \"\"\"\n",
    "    Train multiple models and return the results in a DataFrame.\n",
    "    :param model_names: List with the names of the models to train.\n",
    "    :param X_train: Training set features.\n",
    "    :param X_test: Test set features.\n",
    "    :param y_train: Training set labels.\n",
    "    :param y_test: Test set labels.\n",
    "    :param X: DataFrame with the features.\n",
    "    :return: DataFrame with the results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    ml = ShallowML(X_train, X_test, y_train, y_test, report_name=None, columns_names=X.columns)\n",
    "    for model_name in model_names:\n",
    "        if model_name == 'svm':\n",
    "            model_params = {'clf__C':[0.01, 1.0,10], 'clf__kernel': 'rbf', 'clf__gamma': ['auto', 'scale'], 'clf__class_weight': ['None','balanced']}\n",
    "        elif model_name == 'linear_svm':\n",
    "            model_params={'clf__C':[0.01, 1.0,10], 'clf__penalty': 'l2', 'clf__class_weight': ['None','balanced']}\n",
    "        elif model_name == 'rf':\n",
    "            model_params = {'clf__n_estimators': [10,100,500], 'clf__max_features': [ 'sqrt', 'log2'],'clf__criterion':['gini','entropy'] , 'clf__class_weight': ['None','balanced']}\n",
    "        elif model_name == 'gboosting':\n",
    "            model_params = {'clf__n_estimators': [10, 100, 500], 'clf__max_depth': [1, 3, 5, 10],'clf__max_features': [0.6, 0.9],'clf__learning_rate':[0.1,1],}\n",
    "        elif model_name == 'knn':\n",
    "            model_params = {'clf__n_neighbors': [2,5,10,15],'clf__weights':['uniform', 'distance'], 'clf__leaf_size': [15, 30, 60]}\n",
    "        elif model_name == 'sgd':\n",
    "            model_params = {'clf__loss': ['hinge', 'log', 'modified_huber', 'perceptron'],'clf__alpha': [0.00001, 0.0001, 0.001, 0.01],'clf__early_stopping': [True],'clf__validation_fraction': [0.2],'clf__n_iter_no_change': [5,10],'clf__class_weight':['None','balanced'] }\n",
    "        elif model_name== 'lr':\n",
    "            model_params = {'clf__C': [0.01, 0.1, 1.0, 10.0], 'clf__solver': ['liblinear', 'lbfgs', 'sag'], 'clf__class_weight': ['None','balanced'],'clf__early_stopping': [True],'clf__validation_fraction': [0.2],'clf__n_iter_no_change': [5,10] }\n",
    "        elif model_name== 'nn':\n",
    "            model_params = {'clf__hidden_layer_sizes':[(50,),(100,),(200,)],'clf__activation': ['logistic', 'tanh', 'relu'],'clf__solver':['adam','sgd'],'clf__alpha': [0.00001, 0.0001, 0.001],'clf__learning_rate_init': [0.0001, 0.001, 0.01]}\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} not implemented.\")\n",
    "        # Call the function train best model to train the model\n",
    "        best_classifier = ml.train_best_model(model_name, score =make_scorer(matthews_corrcoef), param_grid=model_params,cv=5)\n",
    "        scores, report, cm, cm2 = ml.score_testset(best_classifier)\n",
    "\n",
    "        results.append({\n",
    "            'model_name': model_name,\n",
    "            'best_params': best_classifier,\n",
    "            'scores': scores\n",
    "        })\n",
    "    \n",
    "    results_df=pd.DataFrame(results)\n",
    "        \n",
    "    return results_df,best_classifier\n",
    "\n",
    "#nao coloquei a dar as melhores features pois o propythia so tem isso implementado para alguns modelos e nao para todos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = train_multiple_models(X_train_descriptors, X_test_descriptors, y_train_descriptors, y_test_descriptors,antioxidant_data, model_names=['rf'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O espaço de busca para a otimização de hiperparâmetros é fornecido pelo argumento param_grid. Se param_grid for None, a função irá buscar um conjunto padrão de hiperparâmetros, que é definido na função _get_opt_params.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn and propythia\n",
    "def create_model(num_conv_layers=2, num_filters=64, filter_sizes=2, pool_size=2, num_dense_layers=2, neurons_dense=128, drop=0.3, activation='relu', last_layers_activations='sigmoid'):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Convolutional layers\n",
    "    for i in range(num_conv_layers):\n",
    "        model.add(tf.keras.layers.Conv1D(filters=num_filters, kernel_size=filter_sizes, strides=1, padding='same', activation=activation))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.MaxPooling1D(pool_size, padding='same'))\n",
    "        model.add(tf.keras.layers.Dropout(drop))\n",
    "\n",
    "    # Flatten layer\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    # Dense layers\n",
    "    for i in range(num_dense_layers):\n",
    "        model.add(tf.keras.layers.Dense(neurons_dense, activation=activation))\n",
    "        model.add(tf.keras.layers.Dropout(drop))\n",
    "    model.add(tf.keras.layers.Dense(1, activation=last_layers_activations))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "dl=DeepML(X_train_nlf, y_train_nlf, X_test_nlf, y_test_nlf, number_classes=2, problem_type='binary',\n",
    "          x_dval=None, y_dval=None, epochs=100, batch_size=64,\n",
    "          path='', report_name=None, verbose=1,\n",
    "         early_stopping_patience=20, reduce_lr_patience=20, reduce_lr_factor=0.2, reduce_lr_min=0.00001,\n",
    "                 )\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "def generate_param_grid(num_conv_layers, num_dense_layers):\n",
    "    param_grid = {\n",
    "        'num_conv_layers': [num_conv_layers],\n",
    "        'num_filters': list(product([32, 64, 128], repeat=num_conv_layers)),\n",
    "        'filter_sizes': [2, 3],\n",
    "        'pool_size': [2, 3],\n",
    "        'num_dense_layers': [num_dense_layers],\n",
    "        'neurons_dense': list(product([32, 64, 128], repeat=num_dense_layers)),\n",
    "        'drop': [0.1, 0.3,  0.5],\n",
    "        'activation': ['relu'],\n",
    "        'last_layers_activations': ['sigmoid']\n",
    "    }\n",
    "\n",
    "    return param_grid\n",
    "    \n",
    "num_conv_layers = 1\n",
    "num_dense_layers = 2\n",
    "param_grid = generate_param_grid(num_conv_layers, num_dense_layers)\n",
    "\n",
    "dl.get_opt_params(param_grid,model,optType='gridSearch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#como ter tudo a correr , sem a necessidade de estar a mudar o numero de camdas convulacionas e dense\n",
    "def generate_param_grids(num_conv_layers, num_dense_layers):\n",
    "    param_grids = []\n",
    "    for num_conv_layers in num_conv_layerse:\n",
    "        for num_dense_layers in num_dense_layers:\n",
    "            param_grid = {\n",
    "                'num_conv_layers': [num_conv_layers],\n",
    "                'num_filters': list(product([32, 64, 128], repeat=num_conv_layers)),\n",
    "                'filter_sizes': [2, 3],\n",
    "                'pool_size': [2, 3],\n",
    "                'num_dense_layers': [num_dense_layers],\n",
    "                'neurons_dense': list(product([32, 64, 128], repeat=num_dense_layers)),\n",
    "                'drop': [0.1, 0.3,  0.5],\n",
    "                'activation': ['relu'],\n",
    "                'last_layers_activations': ['sigmoid']\n",
    "            }\n",
    "            param_grids.append(param_grid)\n",
    "    return param_grids\n",
    "\n",
    "conv_layers_range = range(1, 4)  \n",
    "dense_layers_range = range(1, 4)  \n",
    "param_grids = generate_param_grids(conv_layers_range, dense_layers_range)\n",
    "\n",
    "for param_grid in param_grids:\n",
    "    dl.get_opt_params(param_grid, model, optType='gridSearch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dúvida : como funciona o los na omnia será que tb devo colocar como hiperparametro- acho que não\n",
    "# devo fazer para o cnn-rnn hicrido tendo em conta o tipo de dados ( tabulares e não tabulares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn and propythia\n",
    "def create_rnn_model(rnn_type='LSTM', bidirectional=False, num_rnn_layers=3, hidden_dim=64, num_dense_layers=1, neurons_dense=32, output_dim=1, drop=0.3, activation='relu', last_layers_activations='sigmoid'):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # RNN layers\n",
    "    for i in range(num_rnn_layers):\n",
    "        if rnn_type == 'LSTM':\n",
    "            rnn_layer = tf.keras.layers.LSTM(hidden_dim, return_sequences=(i != num_layers - 1), activation=activation)\n",
    "        elif rnn_type == 'GRU':\n",
    "            rnn_layer = tf.keras.layers.GRU(hidden_dim, return_sequences=(i != num_layers - 1), activation=activation)\n",
    "        elif rnn_type == 'SimpleRNN':\n",
    "            rnn_layer = tf.keras.layers.SimpleRNN(hidden_dim, return_sequences=(i != num_layers - 1), activation=activation)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Supported types are 'LSTM', 'GRU', and 'SimpleRNN'.\")\n",
    "\n",
    "        if bidirectional:\n",
    "            rnn_layer = tf.keras.layers.Bidirectional(rnn_layer)\n",
    "\n",
    "        model.add(rnn_layer)\n",
    "        model.add(tf.keras.layers.Dropout(drop))\n",
    "\n",
    "    # Dense layers\n",
    "    for i in range(num_dense_layers):\n",
    "        model.add(tf.keras.layers.Dense(neurons_dense, activation=activation))\n",
    "        model.add(tf.keras.layers.Dropout(drop))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(output_dim, activation=last_layers_activations))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "dl=DeepML(X_train_nlf, y_train_nlf, X_test_nlf, y_test_nlf, number_classes=2, problem_type='binary',\n",
    "          x_dval=None, y_dval=None, epochs=100, batch_size=64,\n",
    "          path='', report_name=None, verbose=1,\n",
    "         early_stopping_patience=20, reduce_lr_patience=20, reduce_lr_factor=0.2, reduce_lr_min=0.00001,\n",
    "                 )\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "def generate_param_grid(num_rnn_layers, num_dense_layers):\n",
    "    param_grid = {\n",
    "        'rnn_type': ['LSTM', 'GRU', 'SimpleRNN'],\n",
    "        'bidirectional': [True, False],\n",
    "        'num_rnn_layers': [num_rnn_layers],\n",
    "        'hidden_dim': list(product([32, 64, 128],repeat=num_rnn_layers)),\n",
    "        'num_dense_layers': [num_dense_layers],\n",
    "        'neurons_dense': list(product([32, 64, 128], repeat=num_dense_layers)),\n",
    "        'output_dim': [1],\n",
    "        'drop': [0.1, 0.3, 0.5],\n",
    "        'activation': ['relu'],\n",
    "        'last_layers_activations': ['sigmoid']\n",
    "    }\n",
    "\n",
    "    return param_grid\n",
    "num_rnn_layers = 1\n",
    "num_dense_layers = 2\n",
    "param_grid = generate_param_grid(num_rnn_layers, num_dense_layers)\n",
    "\n",
    "dl.get_opt_params(param_grid,model,optType='gridSearch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoencodermlp simultaneously and propythia\n",
    "def create_autoencoder_mlp_model(latent_dim=12, num_layers=2, input_dim=100,neurons_per_layer=50, num_layers_class=2, neurons_mlp=10, num_classes=2, drop=0.3, activation=\"relu\", last_layers_activations=\"sigmoid\"):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Encoder layers\n",
    "    model.add(tf.keras.layers.Dense(input_dim, activation=activation))\n",
    "    model.add(tf.keras.layers.Dropout(drop))\n",
    "    for i in range(1, num_layers):\n",
    "        model.add(tf.keras.layers.Dense(neurons_per_layer, activation=activation))\n",
    "        model.add(tf.keras.layers.Dropout(drop))\n",
    "    model.add(tf.keras.layers.Dense(latent_dim, activation=activation))\n",
    "    model.add(tf.keras.layers.Dropout(drop))\n",
    "\n",
    "    # Decoder layers\n",
    "    for i in reversed(range(num_layers)):\n",
    "        model.add(tf.keras.layers.Dense(neurons_per_layer, activation=activation))\n",
    "        model.add(tf.keras.layers.Dropout(drop))\n",
    "    model.add(tf.keras.layers.Dense(input_dim, activation=activation))\n",
    "\n",
    "    # MLP layers\n",
    "    for i in range(num_layers_class):\n",
    "        model.add(tf.keras.layers.Dense(neurons_mlp, activation=activation))\n",
    "        model.add(tf.keras.layers.Dropout(drop))\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation=last_layers_activations))\n",
    "\n",
    "    model.compile(loss=['mse', 'binary_crossentropy'],loss_weights=[1, 1], optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "dl=DeepML(X_train_nlf, y_train_nlf, X_test_nlf, y_test_nlf, number_classes=2, problem_type='binary',\n",
    "        x_dval=None, y_dval=None, epochs=100, batch_size=64,\n",
    "        path='', report_name=None, verbose=1,\n",
    "        early_stopping_patience=20, reduce_lr_patience=20, reduce_lr_factor=0.2, reduce_lr_min=0.00001,\n",
    "                )\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "def generate_param_grid(num_layers, num_layers_class):\n",
    "    param_grid = {\n",
    "        'latent_dim': [12, 6],\n",
    "        'num_layers': [num_layers],\n",
    "        'input_dim': [100, 200, 300],#devia ser o n_features\n",
    "        'neurons_per_layer': list(product([25, 50, 125]),repeat=num_layers),\n",
    "        'num_layers_class': [num_layers_class],\n",
    "        'neurons_mlp': list(product([10,15,20]),repeat=num_layers_class),\n",
    "        'num_classes': [2],\n",
    "        'drop': [0.1, 0.2, 0.3],\n",
    "        'activation': ['relu'],\n",
    "        'last_layers_activations': ['sigmoid']\n",
    "    }\n",
    "    return param_grid\n",
    "\n",
    "num_layers = 2\n",
    "num_layers_class = 2\n",
    "param_grid = generate_param_grid(num_layers, num_layers_class)\n",
    "\n",
    "dl.get_opt_params(param_grid,model,optType='gridSearch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o hibrido nao vale a pena!! Alias acho que o hibrido nem faz muito sentido no tipo de dados que temos logo tb devemos ter isso em consideração na omnia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
